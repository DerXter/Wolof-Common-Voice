{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddb4dc54-21ed-4dd4-9ad2-ecf813574bf5",
   "metadata": {},
   "source": [
    "# Wolof Data preparation for Common voice recording\n",
    "We use the Wolof part of the data collected by [Masakhane](https://www.masakhane.io/) during the [MasakhaNER project](https://github.com/masakhane-io/lacuna_pos_ner).  \n",
    "We will split them into sentences so that they can be recorded in the best conditions prescribed by [Mozilla](https://commonvoice.mozilla.org/sentence-collector/#/en/how-to)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3fa81a-2382-434b-b649-557e9dc8402f",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca08e962-bad8-4e62-b81f-fc55efe87145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    with open(path, 'r') as file:\n",
    "        return file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93f5c3f8-f04b-4bdd-b6da-26099a254027",
   "metadata": {},
   "outputs": [],
   "source": [
    "wolof_data = read_file(\"data/raw/wolof-masakhane-lacuna-posner.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a173175f-2953-49b0-a806-b0065e89b37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ndekete, cëslaayu yokk gi dañu ko wàññi ba mu gën a tuuti, nguur gi sàkkuwu cee def dara ngir jubbanti juuti bi, ci lu leer ne dayo bi des na ak sàmm njariñal bokk-koppar gi.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wolof_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8790211f-b5c0-414e-b31b-74b1e18c4c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 6366 sentences!\n"
     ]
    }
   ],
   "source": [
    "print(f\"We have {len(wolof_data)} sentences!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ed8410-b5da-46a4-a82a-cd08c7fd3fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Nb of suited sentences: 1621\n",
      "-> Nb of sentences to split: 4745\n"
     ]
    }
   ],
   "source": [
    "final_corpus    = []\n",
    "corpus_to_split = []\n",
    "LIMIT = 14 # Max sentence length suited for common voice requirements\n",
    "\n",
    "for sentence in wolof_data:\n",
    "    sentence_size = len(sentence.split())\n",
    "    if sentence_size > 2 and sentence_size <= LIMIT:\n",
    "        final_corpus.append(sentence)\n",
    "    else:\n",
    "        corpus_to_split.append(sentence)\n",
    "        \n",
    "print(f\"-> Nb of suited sentences: {len(final_corpus)}\")\n",
    "print(f\"-> Nb of sentences to split: {len(corpus_to_split)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "99141b77-feba-424b-bb8b-fcd53300b04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_parts(sentence, limit):\n",
    "    \"\"\" Take a sentence and splited in n parts of size 'limit'\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence_list   = sentence.split()\n",
    "    nb_parts        = list(range(len(sentence_list)//limit))\n",
    "    sentence_splits = \"\"\n",
    "    begin = 0\n",
    "    \n",
    "    for end in nb_parts:\n",
    "        end = (end+1)*limit\n",
    "        sentence_splits += \" \".join(sentence_list[begin:end])+\"\\n\"\n",
    "        begin = end\n",
    "    # After splitting, only keep the rests if its size exceeds 1\n",
    "    if len(sentence_list[begin:]) > 1:\n",
    "        sentence_splits += \" \".join(sentence_list[begin:])+\"\\n\"\n",
    "        nb_parts.append(1) # count the number of splits\n",
    "    return sentence_splits, len(nb_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2ebb241b-c7a4-47cb-b0d2-00ea0c1ebde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Nb of final_corpus_v2: 11987\n"
     ]
    }
   ],
   "source": [
    "final_corpus_v2 = []\n",
    "cp = 0\n",
    "\n",
    "for sentence in corpus_to_split:\n",
    "    splits, tmp = split_into_parts(sentence, LIMIT)\n",
    "    cp += tmp\n",
    "    final_corpus_v2.append(splits)\n",
    "\n",
    "        \n",
    "print(f\"-> Nb of final_corpus_v2: {cp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87500ad2-07c2-4d47-96bc-25f984fb5d0d",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "59d85fcb-4e1e-49a3-9a35-b6653fd28817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean(data):\n",
    "    \"Remove text within parentheses\"\n",
    "    \n",
    "    cleaned = []\n",
    "    for sentence in data:     \n",
    "        sentence = re.sub(r\"\\([^)]*\\)\", \"\", sentence)\n",
    "        sentence = re.sub(r\"\\ +\", \" \", sentence)\n",
    "        cleaned.append(sentence)\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "363ba34e-d887-484d-ba51-50675c6d9049",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_corpus    = clean(final_corpus)\n",
    "final_corpus_v2 = clean(final_corpus_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a8faff-f056-4717-b832-80922888417a",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "35eb74f5-e024-4c8c-a4e5-97287b7efee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_file(file_path, corpus):\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.writelines(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5788fe4c-0cc1-40cc-99e9-0126c14c4f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_file('data/intermediate/wolof_to_upload_part1.txt', final_corpus)\n",
    "to_file('data/intermediate/wolof_to_upload_part2.txt', final_corpus_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bc602192-4ded-4efa-aea2-3db9934ae102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Total Nb of sentences: 13608\n"
     ]
    }
   ],
   "source": [
    "print(f\"-> Total Nb of sentences: {len(final_corpus)+cp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c689e99-ab4f-417b-a4c9-75848c298135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06edcf12-2b19-4f8e-a380-97c70f68c70c",
   "metadata": {},
   "source": [
    "## Splitting the part2 of the corpus into chunks of 2000 sentences\n",
    "It will be easier to share it for crowdsource work.  \n",
    "__NOTE:__ I'll load the exported data part2 because manual post-processing has been done on *wolof_to_upload* files (part 1 & 2) to split sentences, move some sentences from part 2 to 1 and using regex to remove numbers, abbreviations and some punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d616c-4a77-4110-8447-fc83a527701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wolof_data_v2 = read_file(\"wolof_to_upload_part2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb4d81c-e82b-4096-b01e-dea3fd2abd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert wolof_data_v2 to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b389ee4e-a454-4e7b-bb2c-29348364a4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicing(document, path, chunk_prefix='document', edge=300):\n",
    "    \"\"\" This function slices a dataframe document into chunks of size edge\n",
    "    and store the result into the provided path with the chunk_prefix\n",
    "    as the root name of each chunk.\"\"\"\n",
    "    global_size = len(document)\n",
    "    \n",
    "    if path[-1] != \"/\":\n",
    "        path += \"/\"\n",
    "    print(\"Export in progress...\")\n",
    "    for size in tqdm(range(global_size)):\n",
    "        # Split into many files of size 300\n",
    "        if size%300==0:\n",
    "            document[size:edge].to_excel(path+chunk_prefix+\"_\"+str(size)+\".xlsx\", \n",
    "                                         index=False, header=True, engine=\"openpyxl\")\n",
    "        if edge <= global_size-300:\n",
    "            edge+=300\n",
    "        else:\n",
    "            edge = global_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1355fedc-3cbb-4799-a7f9-f867e469da79",
   "metadata": {},
   "outputs": [],
   "source": [
    "slicing(wolof_data_v2, \n",
    "        path=\"data/processed/chunks/\",\n",
    "        chunk_prefix=\"chunk\",\n",
    "        edge=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81363825-4e4e-42f0-a57c-be01c948bc0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
